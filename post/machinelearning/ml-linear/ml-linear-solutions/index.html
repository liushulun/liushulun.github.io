<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
  <link rel="mask-icon" href="/assets/images/favicon-logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/orange/pace-theme-flash.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"luisliu.cn","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12,"reverse":true},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":false,"async":true,"transition":{"menu_item":null,"post_block":"fadeInDown","post_header":null,"post_body":null,"coll_header":null,"sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.json","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="（1）线性回归模型的求解方法（解析法，梯度下降法：① 随机梯度、② SGDRegressor）；（3）次梯度法；（4）坐标轴下降法。">
<meta property="og:type" content="article">
<meta property="og:title" content="ML入门-线性回归三种求解">
<meta property="og:url" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/index.html">
<meta property="og:site_name" content="LuisLiu">
<meta property="og:description" content="（1）线性回归模型的求解方法（解析法，梯度下降法：① 随机梯度、② SGDRegressor）；（3）次梯度法；（4）坐标轴下降法。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/pseudoinverse.png">
<meta property="og:image" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/gradient_demo.png">
<meta property="og:image" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/different_eta.png">
<meta property="og:image" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/subderivative.png">
<meta property="article:published_time" content="2019-01-13T07:21:47.000Z">
<meta property="article:author" content="LuisLiu">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="MachineLearning">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="线性回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/pseudoinverse.png">


<link rel="canonical" href="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/","path":"post/machinelearning/ml-linear/ml-linear-solutions/","title":"ML入门-线性回归三种求解"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ML入门-线性回归三种求解 | LuisLiu</title>
  














<script>
    <!--动态浏览器标签-->
    const ORIGINAL_TITLE = document.title;
    const WELCOME_TITLE = "Welcome Back!";
    const ANIM_TAB_TITLE_HALF_SPACE_CHAR = "\u2002";
    const ANIM_TAB_TITLE_FULL_SPACE_CHAR = "\u3000";
    const ANIM_TAB_TITLE_SHOW_INTERVAL = 50;
    const ANIM_TAB_TITLE_SHOW_STAY = 500;
    const timerSet = new Set();
    document.addEventListener('visibilitychange', function () {
        if (document.hidden) {
            clearTimerSet();

            // $('[rel="icon"]').attr('href', "/images/favicon-32x32-next.png");
            var faviconLink = document.querySelectorAll('[rel="icon"]');
            if (faviconLink != null) {
                faviconLink.href = "/images/favicon-32x32-next.png";
            }
            // 失去焦点时不切换标题
            // document.title = "Waiting...";
            document.title = ORIGINAL_TITLE;
        } else {
            document.title = "";
            showTabTitleAnim(WELCOME_TITLE);
        }
    });

    function showTabTitleAnim(titleText) {
        if (titleText == ORIGINAL_TITLE) {
            clearTimerSet();
            document.title = ORIGINAL_TITLE;
            return;
        }
        const showingLength = (document.title == null) ? 0 : document.title.length;
        if (showingLength >= titleText.length) {
            var timer = setTimeout(function() {
                clearTimerSet();
                document.title = ORIGINAL_TITLE;
            }, ANIM_TAB_TITLE_SHOW_STAY);
            timerSet.add(timer);
        } else {
            const nextChar = titleText[showingLength];
            if (nextChar.trim().length <= 0) {
                // document.title 无法以空格结尾（会被自动删除），导致死循环一直追加空格，必须用专门的空白字符替代。
                document.title += ANIM_TAB_TITLE_HALF_SPACE_CHAR;
                showTabTitleAnim(titleText);
                return;
            }
            document.title += nextChar;
            var timer = setTimeout(function() {
                timerSet.delete(timer);
                showTabTitleAnim(titleText);
            }, ANIM_TAB_TITLE_SHOW_INTERVAL);
            timerSet.add(timer);
        }
    }

    function clearTimerSet() {
        timerSet.forEach(function(value, key, set) {
            clearTimeout(value);
        });
        timerSet.clear();
    }
</script>




<script>
    Pace.options.eventLag = false;
    document.addEventListener("readystatechange", () => {
        if (document.readyState === "interactive") {
            // Do fancy stuff like not pace.js related stuff (animations etc).
        } else if (document.readyState === "complete") {
            // Force all animation to finish.
            Pace.stop();
            // Hide pace elements.
            //document
            //    .querySelector("#pace-content")
            //    .classList.replace("opacity-0", "opacity-100");
        }
    });
</script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">LuisLiu</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">2025/05/22 - 06:09:21</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-th-large fa-fw"></i>About</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">82</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">14</span></a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">153</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>


</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Catalogue
        </li>
        <li class="sidebar-nav-overview">
          Site
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ML%E5%85%A5%E9%97%A8-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%89%E7%A7%8D%E6%B1%82%E8%A7%A3"><span class="nav-text">ML入门-线性回归三种求解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E8%A7%A3%E6%9E%90%E6%B3%95"><span class="nav-text">1. 解析法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-OLS%E6%9C%80%E4%BC%98%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-text">1.1 OLS最优解析解</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-1-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E7%BB%84"><span class="nav-text">1.1.1 正规方程组</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-2-Moore-Penrose%E5%B9%BF%E4%B9%89%E9%80%86"><span class="nav-text">1.1.2 Moore-Penrose广义逆</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Ridge%E6%9C%80%E4%BC%98%E8%A7%A3%E6%9E%90%E8%A7%A3"><span class="nav-text">1.2 Ridge最优解析解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E6%80%BB%E7%BB%93"><span class="nav-text">1.3 总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-text">2. 梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%80%9D%E6%83%B3"><span class="nav-text">2.1 梯度下降法思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A"><span class="nav-text">2.2 梯度下降法数学解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-OLS%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">2.3 OLS的梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Ridge%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">2.4 Ridge的梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Lasso%E6%AC%A1%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-text">2.5 Lasso次梯度法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E5%AE%9E%E7%94%A8Tips"><span class="nav-text">2.6 梯度下降的实用Tips</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-text">2.7 随机梯度下降</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-8-Ridge%E5%92%8CSGDRegressor"><span class="nav-text">2.8 Ridge和SGDRegressor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-8-1-Ridge"><span class="nav-text">2.8.1 Ridge</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-8-2-SGDRegressor"><span class="nav-text">2.8.2 SGDRegressor</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%AC%A1%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-text">3. 次梯度法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%AC%A1%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="nav-text">3.1 什么是次梯度法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E6%A2%AF%E5%BA%A6%E6%B3%95%E4%B8%8E%E6%AC%A1%E6%A2%AF%E5%BA%A6%E6%B3%95%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">3.2 梯度法与次梯度法的区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%9D%90%E6%A0%87%E8%BD%B4%E4%B8%8B%E9%99%8D%E6%B3%95%E6%B1%82%E8%A7%A3"><span class="nav-text">4. 坐标轴下降法求解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Lasso%E5%9D%90%E6%A0%87%E8%BD%B4%E4%B8%8B%E9%99%8D%E7%9A%84%E6%95%B0%E5%AD%A6%E8%A7%A3%E9%87%8A"><span class="nav-text">4.1 Lasso坐标轴下降的数学解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Lasso%E5%9D%90%E6%A0%87%E8%BD%B4%E4%B8%8B%E9%99%8D%E6%AD%A5%E9%AA%A4"><span class="nav-text">4.2 Lasso坐标轴下降步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Scikit-Learn%E4%B8%AD%E7%9A%84Lasso"><span class="nav-text">4.3 Scikit-Learn中的Lasso</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LuisLiu"
      src="/assets/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">LuisLiu</p>
  <div class="site-description" itemprop="description">Keep Learning</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">82</span>
          <span class="site-state-item-name">Posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">Categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">153</span>
        <span class="site-state-item-name">Tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="/" title="Home → &#x2F;" rel="noopener me"><i class="fa fa-home fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/liushulun" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liushulun" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="http://sighttp.qq.com/authd?IDKEY=50d1f9647e1441567ff941b646b66c8053449b2557ca041d" title="Chats → http:&#x2F;&#x2F;sighttp.qq.com&#x2F;authd?IDKEY&#x3D;50d1f9647e1441567ff941b646b66c8053449b2557ca041d" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:luisio@foxmail.com" title="E-Mail → mailto:luisio@foxmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>



<script>
    const ANIM_SIDEBAR_TEXT_SHOW_INTERVAL = 30;
    const ANIM_SIDEBAR_TEXT_SHOW_STAY = 5000;
    const ANIM_SIDEBAR_TEXT_HIDE_INTERVAL = 15;
    const ANIM_SIDEBAR_TEXT_HIDE_STAY = 1000;
    const ANIM_SIDEBAR_CURSOR_INTERVAL = 500;

    const ANIM_SIDEBAR_TEXT_SET = [`Welcome!`,
                       `Feel free.`,
                       `Keep learning!`,
                       `Good luck!`,
                       `Nice to meet you.`,
                       `Does anything interest you?`,
                       `Let's exchange the friend-link!`,
                       `Comment will be available soon.`,
                       `Can't wait to chat with you!`,
                       ];
    var currentAnimText = ANIM_SIDEBAR_TEXT_SET[0];

    function genRandomSidebarText(minVal, maxVal) {
        switch (arguments.length) {
            case 1: {
                return parseInt(Math.random() * minVal, 10); // parseInt 约等于 floor 都是向下取整，第二个参数表示 10 进制
            }
            case 2: {
                return parseInt((Math.random() * (maxVal - minVal)) + minVal, 10);
            }
            default: {
                return 0;
            }
        }
    }

    function startSidebarTextAnim(textNode) {
        if (textNode == null) {
            return;
        }
        showSidebarTextAnim(textNode);
    }

    function startSidebarTextCursorAnim(cursorNode) {
        if (cursorNode == null) {
            return;
        }
        setTimeout(function() {
            if (cursorNode.style.color == 'transparent') {
                cursorNode.style.color = '#999999';
            } else {
                cursorNode.style.color   = 'transparent';
            }
            startSidebarTextCursorAnim(cursorNode);
        }, ANIM_SIDEBAR_CURSOR_INTERVAL);
    }

    function showSidebarTextAnim(textNode) {
        if (textNode == null) {
            return;
        }
        const showingLength = (textNode.innerHTML == null) ? 0 : textNode.innerHTML.length;
        if (showingLength < currentAnimText.length) {
            const nextChar = currentAnimText[showingLength];
            textNode.innerHTML += nextChar;
            if (nextChar.trim().length <= 0) {
                showSidebarTextAnim(textNode);
                return;
            }
            setTimeout(function() {
                showSidebarTextAnim(textNode);
            }, ANIM_SIDEBAR_TEXT_SHOW_INTERVAL);
        } else {
            setTimeout(function() {
                hideSidebarTextAnim(textNode);
            }, ANIM_SIDEBAR_TEXT_SHOW_STAY);
        }
    }

    function hideSidebarTextAnim(textNode) {
        if (textNode == null) {
            return;
        }
        const showingLength = (textNode.innerHTML == null) ? 0 : textNode.innerHTML.length;
        if (showingLength > 0) {
            textNode.innerHTML = textNode.innerHTML.slice(0, -1);
            setTimeout(function() {
                hideSidebarTextAnim(textNode);
            }, ANIM_SIDEBAR_TEXT_HIDE_INTERVAL);
        } else {
            // 所有文字都隐藏后，重新随机选择一个文案。
            // 随机选择时，如果新随机到的文案与上一次的相同，则重新随机一次，最多重试 3 次。
            for (var i = 0; i < 3; i++) {
                let randomTextIndex = genRandomSidebarText(ANIM_SIDEBAR_TEXT_SET.length);
                let randomText = ANIM_SIDEBAR_TEXT_SET[randomTextIndex];
                if (randomText.trim().length <= 0) {
                    continue;
                }
                if (currentAnimText !== randomText) {
                    currentAnimText = randomText;
                    break;
                }
            }
            setTimeout(function() {
                showSidebarTextAnim(textNode);
            }, ANIM_SIDEBAR_TEXT_HIDE_STAY);
        }
    }


    // 创建 Observer，用于监听指定节点及其子节点的所有变化。
    const siteAuthorObserver = new MutationObserver(function() {
        // 找到所有符合 SiteAuthor（包含头像、名称、描述）特征的节点（<div class="site-author">）：
        const allFoundParents = document.getElementsByClassName('site-author');
        if (allFoundParents.length <= 0) {
            // Do nothing, keep observing.
            return;
        }

        // 确保找到的 SiteAuthor 是 Sidebar 内的元素：
        var parentNode = null;
        for (var i = 0; i < allFoundParents.length; i++) {
            const eachNode = allFoundParents[i];
            if (eachNode.parentNode != null && eachNode.parentNode.classList.contains('site-overview-wrap') && eachNode.parentNode.classList.contains('sidebar-panel')) {
                parentNode = eachNode;
                break;
            }
        }
        if (parentNode == null) {
            return;
        }

        siteAuthorObserver.disconnect();

        // 遍历 SiteAuthor 的子元素，找到 SiteDescription 元素：
        var anchorNode = null;
        for (var i = 0; i < parentNode.children.length; i++) {
            const eachNode = parentNode.children[i];
            if (eachNode.classList.contains('site-description')) {
                anchorNode = eachNode;
                break;
            }
        }
        if (anchorNode == null) {
            return;
        }

        // 清空 SiteDescription 元素：
        while (anchorNode.children.length > 0) {
            anchorNode.children[0].remove();
        }
        anchorNode.innerHTML = null;
        anchorNode.innerText = null;

        //
        const containerDiv = document.createElement('div');
        containerDiv.setAttribute('id', "containerDiv");
        containerDiv.setAttribute('style', "padding:5px 0px; margin:0px; text-align:center; display:flex; justify-content:center; line-height:1; font-size:1.0em!important; min-height:1.0em!important;");
        //parent.insertBefore(containerDiv, parent.firstChild);
        anchorNode.insertBefore(containerDiv, anchorNode.firstChild);
        //
        const textDiv = document.createElement('div');
        textDiv.setAttribute('style', "padding:0px; margin:0px;");
        startSidebarTextAnim(textDiv);
        containerDiv.appendChild(textDiv);
        //
        const cursorDiv = document.createElement('div');
        cursorDiv.setAttribute('style', "padding:0px; margin:0px;");
        cursorDiv.innerHTML = '|';
        startSidebarTextCursorAnim(cursorDiv);
        containerDiv.appendChild(cursorDiv);
    });
    siteAuthorObserver.observe(document, {
        childList: true,
        subtree: true
    });

    document.addEventListener('DOMContentLoaded', function() {
        // 所有 DOM(Document Object Model) 加载完成后，不论 siteAuthorObserver 是否监听到，都需要取消监听。
        siteAuthorObserver.disconnect();
    });
</script>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/assets/images/avatar.jpg">
      <meta itemprop="name" content="LuisLiu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LuisLiu">
      <meta itemprop="description" content="Keep Learning">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ML入门-线性回归三种求解 | LuisLiu">
      <meta itemprop="description" content="（1）线性回归模型的求解方法（解析法，梯度下降法：① 随机梯度、② SGDRegressor）；（3）次梯度法；（4）坐标轴下降法。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ML入门-线性回归三种求解
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted:</span>

      <time title="Created:: 2019-01-13 15:21:47" itemprop="dateCreated datePublished" datetime="2019-01-13T15:21:47+08:00">2019-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">Categories:</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MachineLearning/LinearRegression/" itemprop="url" rel="index"><span itemprop="name">LinearRegression</span></a>
        </span>
    </span>

  


  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="far fa-tags"></i>
    </span>
    <span class="post-meta-item-text">Tags:</span>
      <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
        <a href="/tags/AI/" itemprop="url" rel="tag"><span itemprop="name">AI</span></a>
      </span>
        , 
      <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
        <a href="/tags/MachineLearning/" itemprop="url" rel="tag"><span itemprop="name">MachineLearning</span></a>
      </span>
        , 
      <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
        <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="tag"><span itemprop="name">人工智能</span></a>
      </span>
        , 
      <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="tag"><span itemprop="name">机器学习</span></a>
      </span>
        , 
      <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
        <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="url" rel="tag"><span itemprop="name">线性回归</span></a>
      </span>
  </span>


    <span class="post-meta-item" title="Symbols">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">Symbols: </span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="Duration">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">Duration &asymp;</span>
      <span>18 mins.</span>
    </span>
</div>

            <div class="post-description">（1）线性回归模型的求解方法（解析法，梯度下降法：① 随机梯度、② SGDRegressor）；（3）次梯度法；（4）坐标轴下降法。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">




    <script>
        // 目前有一键回顶部和一键去底部按钮，暂时不启用文章的顶部导航：
        // insertHeaderNavigation();

        function insertHeaderNavigation() {
            const postHeaderDiv = document.querySelectorAll('.post-header')[0];
            if (postHeaderDiv == null) {
                return;
            }
            const isNavigationEnabled = "right";
            if (!isNavigationEnabled) {
                return;
            }

            // 不知道为什么，直接获取 post.next 会导致 post.njk 渲染失败，但是又能直接读属性。。。
            
            
            const isNextPostAtRight = ("right" === 'right');
            var leftPostTitle, leftPostPath, rightPostTitle, rightPostPath;
            if (isNextPostAtRight) {
                leftPostTitle = "ML入门-损失和正则的概率解释";
                leftPostPath = "post/machinelearning/ml-linear/ml-linear-loss-regular/";
                rightPostTitle = "ML入门-线性回归简介";
                rightPostPath = "post/machinelearning/ml-linear/ml-linear-introduction/";
            } else {
                leftPostTitle = "ML入门-线性回归简介";
                leftPostPath = "post/machinelearning/ml-linear/ml-linear-introduction/";
                rightPostTitle = "ML入门-损失和正则的概率解释";
                rightPostPath = "post/machinelearning/ml-linear/ml-linear-loss-regular/";
            }
            const leftPostRelatedUrl = "/" + leftPostPath;
            const rightPostRelatedUrl = "/" + rightPostPath;
            if ((leftPostPath == null || leftPostPath.length <= 0) && (rightPostPath == null || rightPostPath.length <= 0)) {
                return;
            }

            var headerNavDiv = document.createElement('div');
            headerNavDiv.className = 'post-nav';
            // 反转 style-border 样式：
            headerNavDiv.style.borderTop = "none";
            headerNavDiv.style.borderBottom = "1px solid #808080";
            // 反转 style-margin 样式：
            headerNavDiv.style.marginTop = "-20px";
            // headerNavDiv.style.marginBottom = "1em";
            headerNavDiv.style.marginBottom = "80px";
            // 反转 style-padding 样式：
            headerNavDiv.style.padding = "0 5px 10px";
            postHeaderDiv.insertBefore(headerNavDiv, postHeaderDiv.firstChild);

            if (leftPostPath != null && leftPostPath.length > 0) {
                var headerNavLeftDiv = document.createElement('div');
                headerNavLeftDiv.className = 'post-nav-item';
                headerNavDiv.append(headerNavLeftDiv);

                var headerNavLeftLink = document.createElement('a');
                headerNavLeftLink.title = leftPostTitle;
                headerNavLeftLink.href = leftPostRelatedUrl;
                headerNavLeftLink.rel = "prev";
                headerNavLeftLink.style.textAlign = "left";
                headerNavLeftDiv.append(headerNavLeftLink);

                var headerNavLeftIcon = document.createElement('i');
                headerNavLeftIcon.className = 'fa fa-angle-left';
                headerNavLeftLink.append(headerNavLeftIcon);

                var headerNavLeftTitleNode = document.createTextNode(leftPostTitle);
                headerNavLeftLink.append(headerNavLeftTitleNode);
            }
            if (rightPostPath != null && rightPostPath.length > 0) {
                var headerNavRightDiv = document.createElement('div');
                headerNavRightDiv.className = 'post-nav-item';
                headerNavDiv.append(headerNavRightDiv);

                var headerNavRightLink = document.createElement('a');
                headerNavRightLink.title = rightPostTitle;
                headerNavRightLink.href = rightPostRelatedUrl;
                headerNavRightLink.rel = "next";
                headerNavRightLink.style.textAlign = "right";
                headerNavRightDiv.append(headerNavRightLink);

                var headerNavRightTitleNode = document.createTextNode(rightPostTitle);
                headerNavRightLink.append(headerNavRightTitleNode);

                var headerNavRightIcon = document.createElement('i');
                headerNavRightIcon.className = 'fa fa-angle-right';
                headerNavRightLink.append(headerNavRightIcon);
            }
        }
    </script>
<span id="more"></span>

<h1 id="ML入门-线性回归三种求解"><a href="#ML入门-线性回归三种求解" class="headerlink" title="ML入门-线性回归三种求解"></a>ML入门-线性回归三种求解</h1><h2 id="1-解析法"><a href="#1-解析法" class="headerlink" title="1. 解析法"></a>1. 解析法</h2><p>在给定正则参数（超参数）λ 的情况下，目标函数的最优解为：$\hat{W} = \arg_W \min J(W)$，满足最优解的必要条件即一阶导数为零：$\dfrac {\partial J(W)} {\partial W} = 0$。</p>
<h3 id="1-1-OLS最优解析解"><a href="#1-1-OLS最优解析解" class="headerlink" title="1.1 OLS最优解析解"></a>1.1 OLS最优解析解</h3><h4 id="1-1-1-正规方程组"><a href="#1-1-1-正规方程组" class="headerlink" title="1.1.1 正规方程组"></a>1.1.1 正规方程组</h4><p>对 OLS 目标函数矩阵形式展开：</p>
<p>$$<br>\begin{aligned}<br>J(W) &amp;= ||y - X W||^2_2 = (y - X W)^T (y - X W)<br>\\<br>&amp;= y^T y - y^T X W - W^T X^T y + W^T X^T X W<br>\end{aligned}<br>$$</p>
<p>根据矩阵的转置运算有：$A^T B = B^T A$，也即 $y^T X W = W^T X^T y$，因此上式等价于：</p>
<p>$$<br>J(W) = y^T y - 2 W^T X^T y + W^T X^T X W<br>$$</p>
<p>因此满足 OLS 最优解即：</p>
<p>$$<br>\begin{aligned}<br>&amp;\dfrac {\partial J(W)} {\partial W} = 0<br>\\<br>\Longrightarrow \quad &amp;\dfrac { \partial (y^T y - 2 W^T X^T y + W^T X^T X W)} { \partial W } = 0 \quad（0 矩阵）<br>\end{aligned}<br>$$</p>
<p>根据矩阵的微分运算有：</p>
<p>$<br>\begin{aligned}<br>&amp;① \quad \dfrac {\partial (A^T B)} {\partial A} = B<br>\\<br>&amp;② \quad \dfrac {\partial (A^T B A)} {\partial A} = (B^T + B) A<br>\end{aligned}<br>$</p>
<p>进而得到下式：</p>
<p>$$<br>0 - 2 X^T y + [ (X^T X)^T + (X^T X) ] W = 0<br>$$</p>
<p>根据矩阵的运算法则有：</p>
<p>$<br>\begin{aligned}<br>\because \quad &amp;X^T X = (X)^T (X^T)^T = (X^T X)^T<br>\\<br>\therefore \quad &amp;X^T X 为对称矩阵<br>\\<br>\Longrightarrow \quad &amp;X^T X = (X^TX)^T<br>\end{aligned}<br>$</p>
<p>进一步合并得：</p>
<p>$$<br>\begin{aligned}<br>&amp;-2 X^T y + 2 X^T X W = 0<br>\\<br>&amp;\Longrightarrow X^T X W = X^T y \quad （正规方程组）<br>\\<br>&amp;\Longrightarrow \hat{W}_{OLS} = (X^T X)^{-1} X^T y<br>\end{aligned}<br>$$</p>
<p>这种求解方式也称为用正规方程组解析求解最小二乘线性回归。但在解析 $ \hat{W}_{OLS} $ 的过程中涉及到了逆矩阵的计算，应当避免。</p>
<h3 id="1-1-2-Moore-Penrose广义逆"><a href="#1-1-2-Moore-Penrose广义逆" class="headerlink" title="1.1.2 Moore-Penrose广义逆"></a>1.1.2 Moore-Penrose广义逆</h3><p>通常，训练的目标是 OLS 目标函数 $J(W) = ||y - XW||^2_2$ 最小，通俗来讲也即 $y$ 与 $X W$ 越接近越好，最好的情况即求解：$y = X W$。</p>
<p>（1）假如 $X$ 为方阵，则可以求其逆：$W = X^{-1} y$，</p>
<p>（2）假如 $X$ 不为方阵，则求其逆矩阵无意义，可求 Moore-Penrose 广义逆：$W = X^+ y$</p>
<blockquote>
<p>广义逆的符号为：</p>
<p><img data-src="./pseudoinverse.png" alt="广义逆符号" title="@ASSET"> </p>
<p>LaTeX 代码为 <code>X^&#123;\dag&#125;</code>，但 Hexo 不支持引入宏包无法显示，因此使用 $X^+$ 代替。</p>
</blockquote>
<p>Moore-Penrose 广义逆可采用奇异值分解（Singular Value Decomposition, SVD）实现：</p>
<p>$<br>\begin{aligned}<br>&amp;若有：X = U \Sigma V^T,<br>\\<br>&amp;其中 U, V 为正交阵，\Sigma 为对角阵（不一定为方阵）<br>\\<br>&amp;则：X^+ = V \Sigma^+ V^T<br>\end{aligned}<br>$</p>
<p>对角阵 $\Sigma$ 求伪逆，则将非零元素求倒数即可：</p>
<p>$$<br>\Sigma = \left(<br>\begin{matrix}<br> \lambda_1 &amp; 0         &amp; \cdots &amp; 0<br> \\<br> 0         &amp; \lambda_2 &amp; \cdots &amp; 0<br> \\<br> \vdots    &amp; \vdots    &amp; \ddots &amp; \vdots<br> \\<br> 0         &amp; 0         &amp; \cdots &amp; 0<br> \\<br>\end{matrix}<br>\right),<br>\Sigma^+ = \left(<br>\begin{matrix}<br> \dfrac 1 \lambda_1 &amp; 0                  &amp; \cdots &amp; 0<br> \\<br> 0                  &amp; \dfrac 1 \lambda_2 &amp; \cdots &amp; 0<br> \\<br> \vdots             &amp; \vdots             &amp; \ddots &amp; \vdots<br> \\<br> 0                  &amp; 0                  &amp; \cdots &amp; 0<br> \\<br>\end{matrix}<br>\right),<br>$$</p>
<p>这也是 Scikit-Learn 中 LinearRegression 推荐的求解方式。</p>
<h3 id="1-2-Ridge最优解析解"><a href="#1-2-Ridge最优解析解" class="headerlink" title="1.2 Ridge最优解析解"></a>1.2 Ridge最优解析解</h3><p>Ridge 比 OLS 多一个 L2 正则，目标函数为：</p>
<p>$$<br>\begin{aligned}<br>J(W) &amp;= ||y - X W||^2_2 + \lambda ||W||^2_2<br>\\<br>&amp;= (y - X W)^T (y - X W) + \lambda W^T W<br>\end{aligned}<br>$$</p>
<p>其最优解也采用 SVD 分解的方式实现。求解偏导数等于零：</p>
<p>$$<br>\begin{aligned}<br>\dfrac {\partial J(W)} {\partial W} &amp;= -2 X^T y + 2 (X^T X) W + 2 \lambda W = 0<br>\\<br>\Longrightarrow \hat{W}_{Ridge} &amp;= (X^T X + \lambda I)^{-1} X^T y<br>\\<br>&amp;（其中 I 为 D \times D 的单位阵）<br>\end{aligned}<br>$$</p>
<p>对比 OLS 的解：</p>
<p>$$<br>\hat{W}_{OLS} = (X^T X)^{-1} X^T y<br>$$</p>
<p>对 Ridge 的解进行变形，配出一个 $\hat{W}_{OLS}$：</p>
<p>$$<br>\begin{aligned}<br>\hat{W}_{Ridge} &amp;= (X^T X + \lambda I)^{-1} X^T y<br>\\<br>&amp;= (X^T X + \lambda I)^{-1} (X^T X) (X^T X)^{-1} X^T y<br>\\<br>&amp;= (X^T X + \lambda I)^{-1} (X^T X) \hat{W}_{OLS}<br>\end{aligned}<br>$$</p>
<p>将 $(X^T X + \lambda I)^{-1}$ 看成是分母，将 $(X^T X)$ 看成分子，由于 $(X^T X + \lambda I) &gt; (X^T X)$，因此有 $\hat{W}_{Ridge} &lt; \hat{W}_{OLS}$。</p>
<p>因此 $\hat{W}_{Ridge} $ 在 $ \hat{W}_{OLS}$ 的基础上进行了收缩，L2 正则也称为权重收缩。</p>
<h3 id="1-3-总结"><a href="#1-3-总结" class="headerlink" title="1.3 总结"></a>1.3 总结</h3><p>（1）OLS 的解为：$\hat{W}_{OLS} = (X^T X)^{-1} X^T y$，需要对矩阵 $X^T X$ 求逆。</p>
<ul>
<li>当输入特征存在共线性（某些特征可以用其他特征的线性组合表示），矩阵 X 是接近不满秩，矩阵 $X^T X$ 接近奇异，求逆不稳定。</li>
</ul>
<p>（2）Ridge 的解为：$\hat{W}_{Ridge} = (X^T X + \lambda I)^{-1} X^T y$，需要对矩阵 $(X^T X + \lambda I)$ 求逆。</p>
<ul>
<li>即使输入特征存在共线性，矩阵 $X$ 不满秩，矩阵 $X^T X$ 对角线存在等于 0 或接近于 0 的元素，但 $0 + \lambda \ne 0$，$(X^T X + \lambda I)$ 求逆仍可得到稳定解。因此岭回归 Ridge 在输入特征存在共线性的情况仍然能得到稳定解。</li>
</ul>
<p>（3）Lasso 无法无法求得解析解，可以用迭代求解。</p>
<hr>
<h2 id="2-梯度下降法"><a href="#2-梯度下降法" class="headerlink" title="2. 梯度下降法"></a>2. 梯度下降法</h2><h3 id="2-1-梯度下降法思想"><a href="#2-1-梯度下降法思想" class="headerlink" title="2.1 梯度下降法思想"></a>2.1 梯度下降法思想</h3><p>解析求解法对 N x D 维矩阵 X 进行 SVD 分解的复杂度为：$O(N^2 D)$。</p>
<ul>
<li>当样本数 N 很大或特征维度 D 很大时，SVD 计算复杂度高，或机器的内存根本不够。</li>
<li>可采用迭代求解的方法：梯度下降法、随机梯度下降法、次梯度法、坐标轴下降法等。</li>
<li>梯度下降法（Gradient Descent）是求解无约束优化问题最常采用的方法之一。</li>
</ul>
<p>在微积分中，一元函数 $f(x)$ 在 $x$ 处的梯度为函数在该点的导数 $\dfrac {df} {dx}$。</p>
<p>对应在多元函数 $f(x_1, ..., x_D)$ 中，在点 $x = (x_1, ..., x_D)$ 处共有 D 个偏导数：$\dfrac {\partial f} {\partial x_1}, ..., \dfrac {\partial f} {\partial x_D}$。将这 D 个偏导数组合成一个 D 维的矢量 $(\dfrac {\partial f} {\partial x_1}, ..., \dfrac {\partial f} {\partial x_D})^T$，即称为函数 $f(x_1, ..., x_D)$ 在点 $x$ 处的梯度，一般记为 $\nabla$ 或 $grad$，即：</p>
<p>$$<br>\nabla f(x_1, ..., x_D) = grad \ f(x_1, ..., x_D) = (\dfrac {\partial f} {\partial x_1}, ..., \dfrac {\partial f} {\partial x_D})^T<br>$$</p>
<p>（1）从几何意义上讲，某点的梯度是函数在该点变化最快的地方。</p>
<p>（2）沿着梯度方向，函数增加最快，更容易找到函数的最大值</p>
<p>（3）沿负梯度方向，函数减少最快，更容易找到函数的最小值。</p>
<blockquote>
<p>$\nabla$ 发音为 nabla，表示微分，不属于希腊字符，只是一个记号。</p>
</blockquote>
<p>正负梯度的例子如下：</p>
<p><img data-src="./gradient_demo.png" alt="梯度示意图" title="@ASSET"></p>
<p>在计算 $f(x)$ 的最小值时，当函数形式比较简单且数据量小，可用解析计算 $ f&#39;(x) = 0 $，否则可用迭代法求解：</p>
<p>（1）从 t = 0 开始，随机寻找一个值 $x^{t = 0}$ 为初始值；</p>
<p>（2）找到下一个点 $x^{t + 1}$，使得函数值越来越小，即 $f(x^{t + 1}) &lt; f(x^t)$；</p>
<p>（3）重复，直到函数值不再见小，则已经找到函数的 <strong>局部极小值</strong>。</p>
<blockquote>
<p>该方法仅能找到局部极小值。</p>
</blockquote>
<p>为此，可以对该迭代方案进行改进：</p>
<ul>
<li>随机寻找初始值时，初始化多个点；</li>
<li>最后从多个局部极小值中取最小的作为最终的极小值。</li>
</ul>
<h3 id="2-2-梯度下降法数学解释"><a href="#2-2-梯度下降法数学解释" class="headerlink" title="2.2 梯度下降法数学解释"></a>2.2 梯度下降法数学解释</h3><p>对函数 $f(x)$ 进行一节泰勒展开得到：</p>
<p>$$<br>f(x + \Delta x) \approx f(x) + \Delta x \nabla f(x)<br>$$</p>
<p>要找到函数的最小值，也即每一次步进 $\Delta x$ 后的函数值均小于原函数值，因此有：</p>
<p>$$<br>\begin{aligned}<br>&amp; f(x + \Delta x) &lt; f(x)<br>\\<br>\Longrightarrow &amp; \Delta x \nabla f(x) &lt; 0<br>\end{aligned}<br>$$</p>
<p>假设令 $\Delta x = - \eta \nabla f(x), \ \ (\eta &gt; 0)$，其中步长 $\eta$ 为一个较小的正数，从而有：</p>
<p>$$<br>\Delta x \nabla f(x) = - \eta \left( \nabla f(x) \right)^2 &lt; 0<br>$$</p>
<p>令 $\Delta x = - \eta \nabla f(x)$ 即可确保 $\left( \nabla f(x) \right)^2 &gt; 0$。</p>
<p>因此，对 $x$ 的更新为：$x^{t + 1} = x + \Delta x = x^t - \eta \nabla f(x)$，也即 $x$ 向负梯度方向 $- \eta \nabla f(x)$ 移动步长 $\eta$，会使得$f(x^{t + 1}) &lt; f(x^t)$，$\eta$ 也称为学习率。</p>
<p>由于只对 $f(x)$ 进行一阶泰勒展开，因此梯度下降法是一阶最优化算法。</p>
<h3 id="2-3-OLS的梯度下降"><a href="#2-3-OLS的梯度下降" class="headerlink" title="2.3 OLS的梯度下降"></a>2.3 OLS的梯度下降</h3><p>OLS 的目标函数为：</p>
<p>$$<br>\begin{aligned}<br>J(W) &amp;= \sum^N_{i = 1} (y_i - W^T X_i)^2<br>\\<br>&amp;= ||y - X W||^2_2<br>\\<br>&amp;= (y - X W)^T (y - X W)<br>\end{aligned}<br>$$</p>
<p>其梯度为：</p>
<p>$$<br>\nabla J(W) = -2 X^T y + 2 X^T X W = -2 X^T (y - X W)<br>$$</p>
<p>梯度下降：</p>
<p>$$<br>\begin{aligned}<br>W^{t + 1} &amp;= W^t - \eta \nabla J(W^t)<br>\\<br>&amp;= W^t + 2 \eta X^T (y - X W^T)<br>\end{aligned}<br>$$</p>
<p>其中 $(y - X W^T)$ 即为预测残差 r，说明参数的更新量与输入 X 和预测残差 r 的相关性有关。$X^T$ 与 r 的相关性较强时需要把 $\eta$ 调大一些，则 r 逐渐与输入 $X^T$ 无关，直到无需再更新 W。</p>
<p>OLS 的梯度下降过程：</p>
<p>（1）从 t = 0 开始，随机寻找一个值 $W^{t = 0}$ 为初始值（或 0）；</p>
<p>（2）计算目标函数 $J(W)$ 在当前值的梯度：$\nabla J(W^t)$；</p>
<p>（3）根据学习率 $\eta$，更新参数：$W^{t + 1} = W^t - \eta \nabla J(W^t)$；</p>
<p>（4）判断是否满足迭代终止条件。若满足，循环结束并返回最佳参数 $W^{t + 1}$ 和目标函数极小值 $J(W^{t + 1})$，否则跳转至第 2 步。</p>
<p>迭代终止条件有：</p>
<p>（1）迭代次数达到预设的最大次数。</p>
<p>（2）迭代过程中目标函数的变化值小于预设值：$\dfrac{J(W^t) - J(W^{t + 1})}{J(W^t)} \le \varepsilon$。</p>
<h3 id="2-4-Ridge的梯度下降"><a href="#2-4-Ridge的梯度下降" class="headerlink" title="2.4 Ridge的梯度下降"></a>2.4 Ridge的梯度下降</h3><p>Ridge 的目标函数为：</p>
<p>$$<br>\begin{aligned}<br>J(W) &amp;= \sum^N_{i = 1} (y_i - W^T X_i)^2 + \lambda \sum^D_{j = 1} w^2_j<br>\\<br>&amp;= ||y - X W||^2_2 + \lambda ||W||^2_2<br>\end{aligned}<br>$$</p>
<p>其梯度为：</p>
<p>$$<br>\nabla J(W) = -2 X^T y + 2 X^T X W + 2 \lambda W<br>$$</p>
<p>Ridge 的梯度下降过程与 OLS 的相同。</p>
<h3 id="2-5-Lasso次梯度法"><a href="#2-5-Lasso次梯度法" class="headerlink" title="2.5 Lasso次梯度法"></a>2.5 Lasso次梯度法</h3><p>Lasso 的目标函数为：</p>
<p>$$<br>\begin{aligned}<br>J(W) &amp;= \sum^N_{i = 1} (y_i - W^T X_i)^2 + \lambda \sum^D_{j = 1} |W_j|<br>\\<br>&amp;= ||y - X W||^2_2 + \lambda ||W||_1<br>\end{aligned}<br>$$</p>
<p>绝对值函数 &amp;||W||_1&amp; 在原点 $W = 0$ 处不可导，无法使用梯度下降求解。</p>
<p>（1）可用次梯度概念替换梯度，得到次梯度法。</p>
<p>（2）或用坐标轴下降求解。</p>
<h3 id="2-6-梯度下降的实用Tips"><a href="#2-6-梯度下降的实用Tips" class="headerlink" title="2.6 梯度下降的实用Tips"></a>2.6 梯度下降的实用Tips</h3><p>（1）梯度下降中的学习率 η 需要小心设置。太大可能引起目标函数震荡，太小收敛速度过慢，可以采用自适应学习率的方案：</p>
<p><img data-src="./different_eta.png" alt="不同 η 的影响" title="@ASSET"></p>
<p>（2）梯度下降对特征的取值范围敏感，建议对输入特征 X 做去量纲处理（可用 sklearn.preprocessing.StandardScaler 实现）：</p>
<p>$$<br>W^{t + 1} = W^t + 2 \eta X^T (y - X W^t) \ \ \ \ （与输入 X 的取值有关）<br>$$</p>
<p>梯度下降算法延伸阅读：<a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" title="@LINK">Introduction to Gradient Descent Algorithm (along with variants) in Machine Learning</a></p>
<h3 id="2-7-随机梯度下降"><a href="#2-7-随机梯度下降" class="headerlink" title="2.7 随机梯度下降"></a>2.7 随机梯度下降</h3><p>在机器学习模型中，目标函数形式为：</p>
<p>$$<br>J(W) = \sum^N_{i = 1} L \left( y_i, f(X_i; W) \right) + \lambda R(W)<br>$$</p>
<p>梯度形式为：</p>
<p>$$<br>\nabla J(W^t) = \sum^N_{i = 1} \nabla L \left( y_i, f(X_i; W^t) \right) + \lambda \nabla R(W^t)<br>$$</p>
<p>当样本中存在信息冗余（正负抵消或梯度相似）时效率不高，因此可以使用随机梯度下降，即每次梯度下降更新时只计算一个样本上的梯度：</p>
<p>$$<br>\nabla J(W^t) = \nabla L \left( y_t, f(X_t; W^t) \right) + \lambda \nabla R(W^t)<br>$$</p>
<p>通俗而言，每一次迭代时，随机选择一个样本，向该样本的负梯度方向移动一步。梯度下降法每一次迭代都需要计算所有样本的梯度，随机梯度下降每一次迭代仅需计算单个样本的梯度：</p>
<p>（1）为了确保收敛，相比于同等条件下的梯度下降，随机梯度下降需要采用更小的步长和更多的迭代轮数。</p>
<p>（2）相比于非随机算法，随机梯度下降在前期的迭代效果卓越。</p>
<p>小批量梯度下降法：介于一次使用所有样本（批处理梯度下降）和一次只是用一个样本（随机梯度下降）之间，也即在随机梯度下降中，每次使用一个小批量的样本代替单个样本。实践中常采用小批量样本（mini-batch）下降。</p>
<blockquote>
<p>随机梯度下降参考文章：</p>
<p>① &quot;Stochastic Gradient Descent&quot; L. Bottou - Website, 2010<br>② &quot;The Tradeoffs of Large Scale Machine Learning&quot; L. Bottou - Website, 2011</p>
</blockquote>
<h2 id="2-8-Ridge和SGDRegressor"><a href="#2-8-Ridge和SGDRegressor" class="headerlink" title="2.8 Ridge和SGDRegressor"></a>2.8 Ridge和SGDRegressor</h2><h3 id="2-8-1-Ridge"><a href="#2-8-1-Ridge" class="headerlink" title="2.8.1 Ridge"></a>2.8.1 Ridge</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class sklearn.linear_model.Ridge</span></span><br><span class="line">Ridge(alpha=<span class="number">1.0</span>,</span><br><span class="line">      fit_intercept=<span class="literal">True</span>,</span><br><span class="line">      normalize=<span class="literal">False</span>,</span><br><span class="line">      copy_X=<span class="literal">True</span>,</span><br><span class="line">      max_iter=<span class="literal">None</span>,</span><br><span class="line">      tol=<span class="number">0.001</span>,</span><br><span class="line">      solver=’auto’,</span><br><span class="line">      random_state=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>（1）其中与优化计算有关的参数如下：</p>
<ul>
<li><p><code>max_iter</code>: 共轭梯度求解器的最大迭代次数。</p>
<p>对于优化算法 solver 为 &#39;sparse_cg&#39; 和 &#39;lsqr&#39;，则默认值由 <code>scipy.sparse.linalg</code> 确定，对于 &#39;sag&#39; 求解器，默认值为 1000。</p>
</li>
<li><p><code>tol</code>: 解的精度，判断迭代收敛与否的阈值。</p>
<p>当（loss &gt; previous_loss - tol）时迭代终止。</p>
</li>
<li><p><code>solver</code>: 求解最优化问题的算法。</p>
<p>可取：&#39;auto&#39;，&#39;svd&#39;，&#39;cholesky&#39;，&#39;lsqr&#39;，&#39;sparse_cg&#39;，&#39;sag&#39;，&#39;saga&#39;。</p>
</li>
<li><p><code>random_state</code>: 数据洗牌时的随机种子。</p>
<p>仅用于 &#39;sag&#39; 求解器。</p>
</li>
</ul>
<p>（2）其中求解器 <code>solver</code> 可选的算法如下：</p>
<ul>
<li><p><code>auto</code>: 根据数据类型自动选择求解器。</p>
<p>默认算法。</p>
</li>
<li><p><code>svd</code>: 使用 X 的奇异值分解来计算 Ridge 系数。</p>
<p>对于奇异矩阵，比 &#39;cholesky&#39; 更稳定。</p>
</li>
<li><p><code>cholesky</code>: 使用标准的 <code>scipy.linalg.solve</code> 函数获得解析解。</p>
</li>
<li><p><code>sparse_cg</code>: 使用 <code>scipy.sparse.linalg.cg</code> 中的共轭梯度求解器。</p>
<p>对大规模数据，比“cholesky”更合适。</p>
</li>
<li><p><code>lsqr</code>: 使用专用的正则化最小二乘常数 <code>scipy.sparse.linalg.lsqr</code>。</p>
<p>速度最快。</p>
</li>
<li><p><code>sag</code>: 使用随机平均梯度下降。</p>
<p>当样本数 n_samples 和特征维数 n_feature 都很大时，通常比其他求解器更快。</p>
</li>
<li><p><code>saga</code>: &#39;sag&#39; 的改进算法。</p>
<p>当 <code>fit_intercept</code> 为 <code>True</code> 时，&#39;sag’ 和 &#39;saga&#39; 只支持稀疏输入。&#39;sag&#39; 和 &#39;saga&#39; 快速收敛仅在具有近似相同尺度的特征上被保证，因此数据需要标准化。</p>
</li>
</ul>
<h4 id="2-8-2-SGDRegressor"><a href="#2-8-2-SGDRegressor" class="headerlink" title="2.8.2 SGDRegressor"></a>2.8.2 SGDRegressor</h4><p>Scikit-Learn 中实现了随机梯度下降回归：SGDRegressor，其对大数据量训练集（n_sample &gt; 10000）的回归问题合适。</p>
<p>SGDRegressor 的目标函数为：</p>
<p>$$<br>J(W) = \dfrac {1} {N} \sum^N_{i = 1} L \left( y_i, f(X_i) \right) + \alpha R(W)<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class sklearn.linear_model.SGDRegressor</span></span><br><span class="line">SGDRegressor(loss=<span class="string">&#x27;squared_loss&#x27;</span>,</span><br><span class="line">             penalty=<span class="string">&#x27;l2&#x27;</span>,</span><br><span class="line">             alpha=<span class="number">0.0001</span>,</span><br><span class="line">             l1_ratio=<span class="number">0.15</span>,</span><br><span class="line">             fit_intercept=<span class="literal">True</span>,</span><br><span class="line">             max_iter=<span class="literal">None</span>,</span><br><span class="line">             tol=<span class="literal">None</span>,</span><br><span class="line">             shuffle=<span class="literal">True</span>,</span><br><span class="line">             verbose=<span class="number">0</span>,</span><br><span class="line">             epsilon=<span class="number">0.1</span>,</span><br><span class="line">             random_state=<span class="literal">None</span>,</span><br><span class="line">             learning_rate=<span class="string">&#x27;invscaling&#x27;</span>,</span><br><span class="line">             eta0=<span class="number">0.01</span>,</span><br><span class="line">             power_t=<span class="number">0.25</span>,</span><br><span class="line">             warm_start=<span class="literal">False</span>,</span><br><span class="line">             average=<span class="literal">False</span>,</span><br><span class="line">             n_iter=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>（1）参数 <code>loss</code> 支持的损失函数包括：</p>
<ul>
<li><code>squared_loss</code>: L2 损失。</li>
<li><code>huber</code>: Huber 损失。</li>
<li><code>epsilon_insensitive</code>: ɛ 不敏感损失 (如：SVM)</li>
<li><code>squared_epsilon_insensitive</code></li>
</ul>
<p>（2）参数 <code>penalty</code> 支持的正则函数包括：</p>
<ul>
<li><code>none</code>: 无正则</li>
<li><code>l2</code>: L2正则</li>
<li><code>l1</code>: L1正则</li>
<li><code>elasticnet</code>: L1 正则 + L2 正则（配合参数 <code>l1_ratio</code> 为 L1 正则的比例）</li>
</ul>
<p>（3）参数 <code>epsilon</code> 是某些损失函数（huber、epsilon_insensitive、squared_epsilon_insensitive）需要的额外参数。</p>
<p>（4）参数 <code>alpha</code> 是正则惩罚系数，也用于学习率计算。</p>
<p>（5）优化算法有关的参数包括：</p>
<ul>
<li><p><code>max_iter</code>: 最大迭代次数（访问训练数据的次数，Epoches 的次数），默认值 5。</p>
<p>一个迭代循环只使用一个随机样本的梯度，并且循环所有的样本，则称为一个 Epoches。SGD 在接近 $10^6$ 的训练样本时收敛。因此可将迭代数设置为 np.ceil($10^6$ / 𝑁)，其中 𝑁 是训练集的样本数目。参数 <code>n_iter</code> 意义相同，已被抛弃。</p>
</li>
<li><p><code>tol</code>: 停止条件。</p>
<p>如果不为 &#39;None&#39;，则当（loss &gt; previous_loss - tol）时迭代终止。</p>
</li>
<li><p><code>shuffle</code>: 每轮 SGD 之前是否重新对数据进行洗牌。</p>
</li>
<li><p><code>random_state</code>: 随机种子，Scikit-Learn 中与随机有关的算法均有此参数，含义相同。</p>
<p>当参数 <code>shuffle == True</code> 时使用。如果随机种子相同，每次洗牌得到的结果一样。可设置为某个整数以复现结果。</p>
</li>
<li><p><code>learning_rate</code>: 学习率。</p>
<p>支持 3 种选择：</p>
<ol>
<li>&#39;constant&#39;：$\eta = \eta_0$。</li>
<li>&#39;optimal&#39;：$\eta = 1.0 / \alpha * (t + t_0)$，分类任务中随机梯度下降默认值。</li>
<li>&#39;invscaling&#39;：$\eta = \eta_0 / pow(t, \ power_t)$，回归任务重随机梯度下降默认值。</li>
</ol>
</li>
<li><p><code>warm_start</code>: 是否从之前的结果继续。</p>
<p>随机梯度下降中初始值可以是之前的训练结果，支持在线学习，即可以在原来的学习基础上继续学习新加入的样本并更新模型参数（输出）。初始值可在 <code>fit</code> 函数中作为参数传递。</p>
</li>
<li><p><code>average</code>: 是否采用平均随机梯度下降法（ASGD）。</p>
</li>
</ul>
<hr>
<h2 id="3-次梯度法"><a href="#3-次梯度法" class="headerlink" title="3. 次梯度法"></a>3. 次梯度法</h2><h3 id="3-1-什么是次梯度法"><a href="#3-1-什么是次梯度法" class="headerlink" title="3.1 什么是次梯度法"></a>3.1 什么是次梯度法</h3><p>当函数可导时，梯度下降法是非常有效的优化算法。但 Lasso 的目标函数为：$J(W) = ||y - X W||^2_2 + \lambda ||W||_1$，其中正则项 $||W||_1$ 为绝对值函数，在 $W_j = 0$ 处不可导，无法计算梯度，也无法用梯度下降法求解。因此需要将梯度扩展为次梯度，用次梯度法求解该问题。</p>
<p>为了处理不平滑函数，扩展导数的表示。定义一个凸函数 $f$ 在点 $x_0$ 处的 <strong>次导数</strong> 为一个标量 g，使得：</p>
<p>$<br>f(x) - f(x_0) \ge g(x - x_0), \forall x \in \mathrm{I}<br>$</p>
<p>其中 $\mathrm{I}$ 为包含 $x_0$ 的某个区间。如下图所示，对于定义域中的任何 $x_0$，总可以做一条直线通过点 $(x_0, f(x_0))$，且直线要么接触 $f$，要么在其下方：</p>
<p><img data-src="./subderivative.png" alt="次导数" title="@ASSET"></p>
<p>上式等价于：</p>
<p>$<br>\Delta f(x) \ge g \Delta x \Rightarrow g \le \dfrac {\Delta f(x)} {\Delta x}<br>$</p>
<p>从该直线方程可知，$g$ 也就是在直线 $y = f(x_0)$ 下方的直线的斜率，所有 $g$ 的解（斜率）都称为函数的次导数（Subderivative），所有次导数（$g$ 的解）的集合称为函数 $f$ 在 $x_0$ 处的次微分（Subdifferential），记为 $\partial f(x_0)$。</p>
<p>次微分是次导数的集合，定义该集合为区间 $[a, b]$：</p>
<p>$$<br>a = \lim_{x \rightarrow x^-_0} \dfrac {f(x) - f(x_0)} {x - x_0}, \quad b = \lim_{x \rightarrow x^+_0} \dfrac {f(x) - f(x_0)} {x - x_0}<br>$$</p>
<p>也即 $x_0$ 点的次微分的集合左边界 $a$ 是从点 $x_0$ 的左侧逼近函数值，右边界 $b$ 是从点 $x_0$ 的右侧逼近函数值。<strong>当函数在 $x_0$ 处可导时，该点的次微分只有一个点组成，也就是函数在该点的导数。</strong></p>
<p>例如求凸函数 $f(x) = |x|$ 的次微分，由于 $f(x)$ 在点 $x = 0$ 处不可导，因此该点的次微分区间左边界为 $f(0^-) = -1$，右边界为 $f(0^+) = 1$：</p>
<p>$$<br>\partial f(x) = \left \{<br>\begin{aligned}<br>\{ -1 \}, &amp;&amp; {x &lt; 0}<br>\\<br>[-1, +1], &amp;&amp; {x = 0}<br>\\<br>\{ +1 \}, &amp;&amp; {x &gt; 0}<br>\end{aligned}<br>\right.<br>$$</p>
<p>若求解多维点的次微分，则分别求解每个分量的次微分并组成向量，即作为函数在该点的次梯度。</p>
<h3 id="3-2-梯度法与次梯度法的区别"><a href="#3-2-梯度法与次梯度法的区别" class="headerlink" title="3.2 梯度法与次梯度法的区别"></a>3.2 梯度法与次梯度法的区别</h3><p>对可导函数，最优解的条件为 $f(x) = 0$，对此类仅局部可导，需要使用次微分的函数，最优解的条件为：</p>
<p>$<br>0 \in \partial f(x^<em>) \Longleftrightarrow f(x^</em>) = \min_x f(x)<br>$</p>
<p>当且仅当 0 属于函数 $f$ 在点 $x^<em>$ 处次梯度集合时，$x^</em>$ 为极值点。当然，因为函数在可导的点的次微分等于其导数，因此该条件可扩展到全局可导函数。</p>
<blockquote>
<p>Python 可用 <code>numpy.sign</code> 函数实现绝对值函数的次梯度。</p>
</blockquote>
<p>将梯度下降法中的梯度换成次梯度就得到次梯度法：</p>
<table>
<thead>
<tr>
<th align="left">梯度下降法</th>
<th align="left">次梯度法</th>
</tr>
</thead>
<tbody><tr>
<td align="left">1. 从 $t = 0$ 开始，初始化 $w^0$</td>
<td align="left">1. 从 $t = 0$ 开始，初始化 $w^0$</td>
</tr>
<tr>
<td align="left">2. 计算目标函数 $J(W)$ 在当前值的梯度：$\nabla J(W^t)$</td>
<td align="left">2. 计算目标函数 $J(W)$ 在当前值的次梯度：$\partial J(W^t)$</td>
</tr>
<tr>
<td align="left">3. 根据学习率 $\eta$ 更新参数：$W^{t + 1} = W^t - \eta \nabla J(W^t)$</td>
<td align="left">3. 根据学习率 $\eta$ 更新参数：$W^{t + 1} = W^t - \eta \partial J(W^t)$</td>
</tr>
<tr>
<td align="left">4. 判断是否满足迭代总之条件，如果满足，循环结束并返回最佳参数 $W^{t + 1}$ 和目标函数极小值 $J(W^{t+ 1})$，否则跳转到第 2 步</td>
<td align="left">判断是否满足迭代总之条件，如果满足，循环结束并返回最佳参数 $W^{t + 1}$ 和目标函数极小值 $J(W^{t+ 1})$，否则跳转到第 2 步</td>
</tr>
</tbody></table>
<p><strong>结论：</strong></p>
<p>与梯度下降算法不同，次梯度算法并不是下降算法（每次对参数的更新，并不能保证目标函数单调递减）。因此一般情况下会从多个点同时应用次梯度法，最后选择最小值：</p>
<p>$$<br>f(x^*) = \min_{1, ..., t} f(x^t)<br>$$</p>
<p>虽然次梯度法不能保证迭代过程中目标函数保持单调下降，但可以证明，满足一定条件的凸函数，次梯度法可以保证收敛，只是收敛速度比梯度下降法慢。因此 Lasso 通常使用 <strong>坐标轴下降法</strong> 求解。</p>
<hr>
<h2 id="4-坐标轴下降法求解"><a href="#4-坐标轴下降法求解" class="headerlink" title="4. 坐标轴下降法求解"></a>4. 坐标轴下降法求解</h2><p>次梯度法收敛速度慢，Lasso 求解推荐使用坐标轴下降法。</p>
<p>坐标轴下降法即：沿着坐标轴方向搜索。和梯度下降法与随机梯度下降法的概念类似，例如对 D 维样本参数 $W_0, ..., W_D$，坐标轴下降法是每次仅对其中一个 $W_j$ 搜索最优值。循环使用不同的坐标轴（不同维度），一个周期的以为搜索迭代过程相当于一个梯度迭代。</p>
<p>坐标轴下降发利用当前坐标系统进行搜索，无需计算目标函数的导数，只按照某一坐标方向进行搜索最小值，而梯度下降法验目标函数的负梯度方向搜索，因此梯度方向通常不与任何坐标轴平行。</p>
<p>坐标轴下降法在系数矩阵上的计算速度非常快。</p>
<h3 id="4-1-Lasso坐标轴下降的数学解释"><a href="#4-1-Lasso坐标轴下降的数学解释" class="headerlink" title="4.1 Lasso坐标轴下降的数学解释"></a>4.1 Lasso坐标轴下降的数学解释</h3><p>Lasso 的目标函数为：$J(W) = ||y - X W||^2_2 + \lambda ||W||_1$。</p>
<p>将 Lasso 目标函数中的损失和及正则项分别应用坐标轴下降法搜索，每次仅搜索一个维度。定义 $w_{-j}$ 为 $W$ 去掉 $w_j$ 后的剩余 $(D - 1)$ 维向量。</p>
<p>（1）对 RSS 的第 j 维坐标轴下降（可导，直接计算梯度）：</p>
<p>$$<br>\begin{aligned}<br>\dfrac {\partial} {\partial w_j} RSS(W) &amp;= \dfrac {\partial} {\partial w_j} \sum^N_{i = 1} (y_i - W^T X_i)^2<br>\\<br>&amp;= \dfrac {\partial} {\partial w_j} \sum^N_{i = 1} (y_i - (W^T_{-j} X_{i, -j} + w_j x_{ij}))^2<br>\\<br>&amp;= \dfrac {\partial} {\partial w_j} \sum^N_{i = 1} (y_i - W^T_{-j} X_{i, -j} - w_j x_{ij})^2<br>\\<br>（复合函数求导） &amp;= -2 \sum^N_{i = 1} (y_i - W^T_{-j} X_{i, -j} - w_i x_{ij}) \cdot x_{ij}<br>\\<br>&amp;= 2 \sum^N_{i = 1} x^2_{ij} w_j - 2 \sum^N_{i = 1} x_{ij} (y_i - W^T_{-j} X_{i, -j})<br>\\ \\<br>令：a_j &amp;= 2 \sum^N_{i = 1} x^2_{ij}, \quad c_j =  \sum^N_{i = 1} x_{ij} (y_i - W^T_{-j} X_{i, -j})<br>\\ \\<br>\Longrightarrow \dfrac {\partial} {\partial w_j} RSS(W) &amp;= a_j w_j - c_j<br>\end{aligned}<br>$$</p>
<p>（2）再对 $R(W)$ 的第 j 维坐标轴下降（计算次梯度）：</p>
<p>$$<br>\dfrac {\partial} {\partial w_j} R(W) = \dfrac {\partial} {\partial w_j} \lambda |w_j| =<br>\left \{<br>\begin{aligned}<br>&amp; \dfrac {\partial} {\partial w_j} (- w_j \lambda) = \{ - \lambda \}, &amp;&amp; {w_j &lt; 0}<br>\\<br>&amp; [- \lambda, + \lambda], &amp;&amp; {w_j = 0}<br>\\<br>&amp; \dfrac {\partial} {\partial w_j} (w_j \lambda) = \{ \lambda \}, &amp;&amp; {w_j &gt; 0}<br>\end{aligned}<br>\right.<br>$$</p>
<p>（3）合并为对 $J(W, \lambda)$ 的第 j 维坐标轴下降：</p>
<p>$$<br>\begin{aligned}<br>&amp; \dfrac {\partial} {\partial w_j} J(W, \lambda) = \dfrac {\partial} {\partial w_j} (RSS + R(W))<br>\\<br>&amp;= \left \{<br>\begin{aligned}<br>&amp; \{ a_j w_j - c_j - \lambda \}, &amp;&amp; {w_j &lt; 0}<br>\\<br>&amp; [a_j w_j - c_j - \lambda, a_j w_j - c_j + \lambda] = [- c_j - \lambda, - c_j + \lambda], &amp;&amp; {w_j = 0}<br>\\<br>&amp; \{ a_j w_j - c_j + \lambda \}, &amp;&amp; {w_j &gt; 0}<br>\end{aligned}<br>\right.<br>\end{aligned}<br>$$</p>
<p>（4）最优解需满足：$0 \in \dfrac {\partial} {\partial w_j} J(W, \lambda)$，对于可导部分，则为 $0 = \dfrac {\partial} {\partial w_j} J(W, \lambda)$：</p>
<p>$$<br>\Longrightarrow \left \{<br>\begin{aligned}<br>&amp; 0 = a_j w_j - c_j - \lambda, &amp;&amp; {w_j &lt; 0}<br>\\<br>&amp; 0 \in [- c_j - \lambda, - c_j + \lambda], &amp;&amp; {w_j = 0}<br>\\<br>&amp; 0 = a_j w_j - c_j + \lambda, &amp;&amp; {w_j &gt; 0}<br>\end{aligned}<br>\right.<br>$$</p>
<p>其中：</p>
<p>$<br>\begin{aligned}<br>&amp; 0 \in [- c_j - \lambda, - c_j + \lambda]<br>\\<br>&amp; \Longrightarrow \left \{<br>\begin{aligned}<br>0 \ge - c_j - \lambda<br>\\<br>0 \le - c_j + \lambda<br>\end{aligned}<br>\right.<br>\\<br>&amp; \Longleftrightarrow c_j \in [- \lambda, \lambda]<br>\end{aligned}<br>$</p>
<p>由于 $a_j = 2 \sum^N_{i = 1} x^2_{ij} &gt; 0$，</p>
<p>① 当 $w_j &lt; 0$ 时有：</p>
<p>$<br>\begin{aligned}<br>&amp; 0 = a_j w_j - c_j - \lambda<br>\\<br>&amp; w_j = \dfrac {c_j + \lambda} {a_j} &lt; 0<br>\\<br>&amp; \Longrightarrow c_j &lt; - \lambda<br>\end{aligned}<br>$</p>
<p>② 同理，当 $w_j &gt; 0$ 时有：</p>
<p>$<br>\begin{aligned}<br>&amp; 0 = a_j w_j - c_j + \lambda<br>\\<br>&amp; w_j = \dfrac {c_j - \lambda} {a_j} &gt; 0<br>\\<br>&amp; \Longrightarrow c_j &gt; \lambda<br>\end{aligned}<br>$</p>
<p>因此可以转换为下式：</p>
<p>$$<br>\hat{w_j}(c_j) = \left \{<br>\begin{aligned}<br>&amp; \dfrac {c_j + \lambda} {a_j}, &amp;&amp; {c_j &lt; - \lambda}<br>\\<br>&amp; 0, &amp;&amp; {c_j \in [- \lambda, \lambda]}<br>\\<br>&amp; \dfrac {c_j - \lambda} {a_j}, &amp;&amp; {c_j &gt; \lambda}<br>\end{aligned}<br>\right.<br>$$</p>
<h3 id="4-2-Lasso坐标轴下降步骤"><a href="#4-2-Lasso坐标轴下降步骤" class="headerlink" title="4.2 Lasso坐标轴下降步骤"></a>4.2 Lasso坐标轴下降步骤</h3><p>由于 $a_j = 2 \sum^N_{i = 1} x^2_{ij}$ 对于已知的输入 $X$ 是可以预计算的，因此 Lasso 坐标轴下降的步骤如下：</p>
<p>① 预计算 $a_j = 2 \sum^N_{i = 1} x^2_{ij}$</p>
<p>② 初始化参数 $W$（全 0 或随机）</p>
<p>③ 选择变化幅度最大的维度、或随机选择、或轮流选择需要更新的参数 $w_j$</p>
<p>④ 计算 $c_j =  \sum^N_{i = 1} x_{ij} (y_i - W^T_{-j} X_{i, -j})$</p>
<p>⑤ 计算 $\hat{w_j}(c_j) = \left \{<br>\begin{aligned}<br>&amp; \dfrac {c_j + \lambda} {a_j}, &amp;&amp; {c_j &lt; - \lambda}<br>\\<br>&amp; 0, &amp;&amp; {c_j \in [- \lambda, \lambda]}<br>\\<br>&amp; \dfrac {c_j - \lambda} {a_j}, &amp;&amp; {c_j &gt; \lambda}<br>\end{aligned}<br>\right.$</p>
<p>⑥ 重复第 3 ~ 5 步直到收敛</p>
<p>⑦ 根据训练好的 $W$ 调整 $\lambda$ 的取值。</p>
<p><strong>注意：</strong></p>
<p>$c_j =  \sum^N_{i = 1} x_{ij} (y_i - W^T_{-j} X_{i, -j})$，其中的 $W^T_{-j} X_{i, -j}$ 本质上是分别从 $W$ 和 $X$ 中各去掉了一维后的向量/矩阵相乘，但从另一方面理解，也可以认为是去掉了第 j 维特征后用剩下的特征计算出来的预测值，因此 $y_i - W^T_{-j} X_{i, -j}$ 实际上也是第 i 个样本的预测残差 $r_i$，而 $c_j = X_j \cdot r$ 可以表示输入特征 $X$ 和预测残差 $r$ 的相关性。</p>
<p><strong>（1）当特征与预测残差强相关时，表示该输入特征的取值（实际上由权重 $w_j$ 控制）对预测结果（残差）有很大影响（例如 $r_j$ 下降很快），则说明这个特征很重要（即权重 $w_j$ 是必须的）。</strong></p>
<p><strong>（2）当特征与预测残差弱相关时，则表示有没有该特征对预测结果没有什么影响，因此直接使得 $w_j = 0$，这也是 L1 正则起到特征选择作用的原理。</strong></p>
<p><strong>（3）这也印证了目标函数 $J(W, \lambda) = \sum Loss + \lambda R(W)$ 中正则参数 $\lambda$ 的理解：$\lambda$ 为正则项的惩罚，$\lambda$ 越大，对应的 $[ -\lambda, \lambda]$ 区间也越宽，则 $w_j = 0$ 的可能性越大，因此得到的解越稀疏，从而 $W$ 的复杂度越低。</strong></p>
<p><strong>（4）是否 $c_j \in [- \lambda, \lambda]$ 决定了 $w_j$ 是否为 0，而 $c_j$ 同样表示输入特征和预测残差之间的相关性。当 $\lambda$ 大于某个最值时，会导致所有的权重均为零 $w_j = 0$。这个最大值同样是可以预计算的：当 $\lambda$ 取最大值时，所有权重均为零，因此每条样本的预测值全为 0，对应的每条样本的预测残差即为真实值本身：$r_i = y_i$，因此 $c_j$ 即可用输入特征和真实值的相关性来代替：$c_j = X^T_{: j} y$，其中 $X_{: j}$ 表示所有样本的第 j 维特征值，因此当 $\lambda \ge \max_j (X^T_{: j} y)$ 时，可得所有 $w_j = 0$。</strong></p>
<h3 id="4-3-Scikit-Learn中的Lasso"><a href="#4-3-Scikit-Learn中的Lasso" class="headerlink" title="4.3 Scikit-Learn中的Lasso"></a>4.3 Scikit-Learn中的Lasso</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># class sklearn.linear_model.Lasso</span></span><br><span class="line">Lasso(alpha=<span class="number">1.0</span>,</span><br><span class="line">      fit_intercept=<span class="literal">True</span>,</span><br><span class="line">      normalize=<span class="literal">False</span>,</span><br><span class="line">      precompute=<span class="literal">False</span>,</span><br><span class="line">      copy_X =<span class="literal">True</span>,</span><br><span class="line">      max_iter=<span class="number">1000</span>,</span><br><span class="line">      tol=<span class="number">0.0001</span>,</span><br><span class="line">      warm_start=<span class="literal">False</span>,</span><br><span class="line">      positive=<span class="literal">False</span>,</span><br><span class="line">      random_state=<span class="literal">None</span>,</span><br><span class="line">      selection=<span class="string">&#x27;cyclic&#x27;</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li><p><code>precompute</code>: 是否使用预计算的 Gram 矩阵来加速计算。</p>
<p>可取值：&#39;True&#39;, &#39;False&#39;, ‘auto’ 或数组（array-like）。若设置为 &#39;auto&#39; 则由机器决定。</p>
</li>
<li><p><code>max_iter</code>: 最大迭代次数。</p>
</li>
<li><p><code>tol</code>: 解的精度，判断迭代收敛与否的阈值。</p>
<p>当更新量小于tol时，优化代码检查优化的 dual gap 并继续直到小于 tol 为止。</p>
</li>
<li><p><code>warm_start</code>: 是否从之前的结果继续。</p>
<p>初始值可以是之前的训练结果，支持在线学习。初始值可在 fit 函数中作为参数传递。</p>
</li>
<li><p><code>positive</code>: 是否强制使系数 $W$ 为正。</p>
</li>
<li><p><code>random_state</code>: 随机选择特征的权重进行更新的随机种子。</p>
<p>仅当参数 <code>selection == &#39;random&#39;</code> 时有效。</p>
</li>
<li><p><code>selection</code>: 选择特征权重更新的方式。</p>
<p>可选项有：</p>
<ol>
<li>&#39;cyclic&#39;：循环更新</li>
<li>&#39;random&#39;：随机选择特征进行更新，通常收敛更快，尤其当参数 tol &gt; (10 - 4) 时。</li>
</ol>
</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">

          <div class="reward-container">
  <div> </div>
  <button>
    支持一下
  </button>
  <div class="post-reward">
      <div>
        <img src="/assets/images/reward.png" alt="LuisLiu WeChat Pay">
        <span>WeChat Pay</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>Post author:  </strong>LuisLiu
  </li>
  <li class="post-copyright-link">
      <strong>Post link: </strong>
      <a href="https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/" title="ML入门-线性回归三种求解">https://luisliu.cn/post/machinelearning/ml-linear/ml-linear-solutions/</a>
  </li>
  <li class="post-copyright-license">
      <strong>Copyright Notice:  </strong>All posts are written by Luis and licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a>, please get authorized before any references.
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/AI/" rel="tag"><i class="fa fa-tag"></i> AI</a>
              <a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"><i class="fa fa-tag"></i> 人工智能</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" rel="tag"><i class="fa fa-tag"></i> 线性回归</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/post/machinelearning/ml-linear/ml-linear-loss-regular/" rel="prev" title="ML入门-损失和正则的概率解释">
                  <i class="fa fa-angle-left"></i> ML入门-损失和正则的概率解释
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/post/machinelearning/ml-linear/ml-linear-introduction/" rel="next" title="ML入门-线性回归简介">
                  ML入门-线性回归简介 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
  <div class="languages">
    <label class="lang-select-label">
      <i class="fa fa-language"></i>
      <span>English</span>
      <i class="fa fa-angle-up" aria-hidden="true"></i>
    </label>
    <select class="lang-select" data-canonical="" aria-label="Select language">
      
        <option value="en" data-href="/post/machinelearning/ml-linear/ml-linear-solutions/" selected="">
          English
        </option>
      
        <option value="zh-CN" data-href="/zh-CN/post/machinelearning/ml-linear/ml-linear-solutions/" selected="">
          简体中文
        </option>
      
    </select>
  </div>

  <div class="copyright">
    &copy; 2018 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa-solid fa-at"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">LuisLiu</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>Symbols: </span>
    <span title="Symbols">517k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span>Duration &asymp;</span>
    <span title="Duration">9:35</span>
  </span>
</div>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/liushulun" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.31/fancybox/fancybox.umd.js" integrity="sha256-a+H7FYzJv6oU2hfsfDGM2Ohw/cR9v+hPfxHCLdmCrE8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>




<style type="text/css">
    .toBottom {
        color: #EEEEEE;
        border-bottom: none;
    }
    .toBottom:hover {
        color: #EB6D39;
        border-bottom: none;
    }
</style>





<div class="scrollToBottom back-to-top back-to-top-on" role="button" style="bottom: 61px;">
    <i class="fa fa-arrow-down fa-lg"></i>
    
</div>
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const scrollToBottom = document.querySelector('.scrollToBottom');
        /* 内联写法：
        $('.scrollToBottom').click(function(){
            $('html,body').animate({scrollTop:document.body.scrollHeight}, 500);
        });
        */
        scrollToBottom && scrollToBottom.addEventListener('click', () => {
            window.anime({
            targets  : document.scrollingElement,
            duration : 500,
            easing   : 'linear',
            scrollTop: document.scrollingElement.scrollHeight
            });
        });
    });
</script>

<a id='postBottom'></a>

</body>
</html>
